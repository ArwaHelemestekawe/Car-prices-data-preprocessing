{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as  plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.130000e+05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Shoreline</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.384000e+06</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3370.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.420000e+05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kent</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200000e+05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.500000e+05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>3.081667e+05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>5.343333e+05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>4.169042e+05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3010.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3010.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Renton</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>2.034000e+05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2090.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>2.206000e+05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Covington</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4600 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             price  bedrooms  bathrooms  sqft_living  floors  waterfront  \\\n",
       "0     3.130000e+05       3.0       1.50       1340.0     1.5         0.0   \n",
       "1     2.384000e+06       5.0       2.50       3650.0     2.0         0.0   \n",
       "2     3.420000e+05       3.0       2.00       1930.0     1.0         0.0   \n",
       "3     4.200000e+05       3.0       2.25       2000.0     1.0         0.0   \n",
       "4     5.500000e+05       4.0       2.50       1940.0     1.0         0.0   \n",
       "...            ...       ...        ...          ...     ...         ...   \n",
       "4595  3.081667e+05       3.0       1.75       1510.0     1.0         0.0   \n",
       "4596  5.343333e+05       3.0       2.50       1460.0     2.0         0.0   \n",
       "4597  4.169042e+05       3.0       2.50       3010.0     2.0         0.0   \n",
       "4598  2.034000e+05       4.0       2.00       2090.0     1.0         0.0   \n",
       "4599  2.206000e+05       3.0       2.50       1490.0     2.0         0.0   \n",
       "\n",
       "      view  sqft_above  sqft_basement       city country  \n",
       "0      0.0      1340.0            0.0  Shoreline     USA  \n",
       "1      4.0      3370.0          280.0    Seattle     USA  \n",
       "2      0.0      1930.0            0.0       Kent     USA  \n",
       "3      0.0      1000.0         1000.0   Bellevue     USA  \n",
       "4      0.0      1140.0          800.0    Redmond     USA  \n",
       "...    ...         ...            ...        ...     ...  \n",
       "4595   0.0      1510.0            0.0    Seattle     USA  \n",
       "4596   0.0      1460.0            0.0   Bellevue     USA  \n",
       "4597   0.0      3010.0            0.0     Renton     USA  \n",
       "4598   0.0      1070.0         1020.0    Seattle     USA  \n",
       "4599   0.0      1490.0            0.0  Covington     USA  \n",
       "\n",
       "[4600 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_prices=pd.read_csv(\"housing_prices.csv\")\n",
    "home_prices\n",
    "\n",
    "# car prices مفهاش price :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      bedrooms  bathrooms  sqft_living  floors  waterfront  view  sqft_above  \\\n",
      "0          3.0       1.50       1340.0     1.5         0.0   0.0      1340.0   \n",
      "1          5.0       2.50       3650.0     2.0         0.0   4.0      3370.0   \n",
      "2          3.0       2.00       1930.0     1.0         0.0   0.0      1930.0   \n",
      "3          3.0       2.25       2000.0     1.0         0.0   0.0      1000.0   \n",
      "4          4.0       2.50       1940.0     1.0         0.0   0.0      1140.0   \n",
      "...        ...        ...          ...     ...         ...   ...         ...   \n",
      "4595       3.0       1.75       1510.0     1.0         0.0   0.0      1510.0   \n",
      "4596       3.0       2.50       1460.0     2.0         0.0   0.0      1460.0   \n",
      "4597       3.0       2.50       3010.0     2.0         0.0   0.0      3010.0   \n",
      "4598       4.0       2.00       2090.0     1.0         0.0   0.0      1070.0   \n",
      "4599       3.0       2.50       1490.0     2.0         0.0   0.0      1490.0   \n",
      "\n",
      "      sqft_basement  \n",
      "0               0.0  \n",
      "1             280.0  \n",
      "2               0.0  \n",
      "3            1000.0  \n",
      "4             800.0  \n",
      "...             ...  \n",
      "4595            0.0  \n",
      "4596            0.0  \n",
      "4597            0.0  \n",
      "4598         1020.0  \n",
      "4599            0.0  \n",
      "\n",
      "[4600 rows x 8 columns]\n",
      "             price\n",
      "0     3.130000e+05\n",
      "1     2.384000e+06\n",
      "2     3.420000e+05\n",
      "3     4.200000e+05\n",
      "4     5.500000e+05\n",
      "...            ...\n",
      "4595  3.081667e+05\n",
      "4596  5.343333e+05\n",
      "4597  4.169042e+05\n",
      "4598  2.034000e+05\n",
      "4599  2.206000e+05\n",
      "\n",
      "[4600 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nHowever, random splitting isn’t always the best choice. For example,\\nif you have a small dataset, or if your data is imbalanced (i.e., one class has many more samples than another),\\nthen simple random splitting may result in training or testing sets that don’t accurately represent \\nthe overall distribution of data1. In such cases, other methods like stratified sampling might be more appropriate1.\\nAlso, when dealing with time series data (like stock prices or weather data),\\nit’s often more appropriate to split the data based on a *chronological* order rather than randomly1.\\nThis helps in evaluating the model’s performance on future unseen dat\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = home_prices.drop([\"price\",\"city\",\"country\"],axis =1)\n",
    "y = home_prices[\"price\"]\n",
    "\n",
    "\n",
    "\n",
    "# Trying to getrid of all null values by filling it with mean\n",
    "\n",
    "df_x =pd.DataFrame(x)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Fit and transform the features in the DataFrame\n",
    "df_imputed_x = imputer.fit_transform(df_x)\n",
    "\n",
    "\n",
    "# Convert the result back into a DataFrame\n",
    "df_imputed_final_x = pd.DataFrame(df_imputed_x, columns=df_x.columns)\n",
    "\n",
    "print(df_imputed_final_x)\n",
    "\n",
    "\n",
    "\n",
    "df_y =pd.DataFrame(y)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "df_imputed_y = imputer.fit_transform(df_y)\n",
    "\n",
    "df_imputed_final_y = pd.DataFrame(df_imputed_y, columns=df_y.columns)\n",
    "\n",
    "print(df_imputed_final_y)\n",
    "\n",
    "\n",
    "'''\n",
    "In this code, SimpleImputer is initialized with the strategy of replacing missing values (np.nan) with the mean of the column. The fit_transform method is then used to apply this strategy to the DataFrame df. The result is a new DataFrame where all missing values have been\n",
    " replaced with the mean of their respective column.\n",
    "\n",
    "'''\n",
    "y = home_prices[\"price\"]\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(df_imputed_final_x,df_imputed_final_y,test_size=.2,random_state=42)\n",
    "\n",
    "\n",
    "'''\n",
    "iam  pretty confused about random spliting ...  i think random spliting can cuase data leakage , may be some of \n",
    "data which may have to be in test section now in the train it may cuase over fitting...\n",
    "\n",
    "**Use a stratified split algorithm: Especially for small datasets or when you work with highly imbalanced data.\n",
    " Stratification guarantees that each split contains approximately the same number or distribution of classes***\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "However, random splitting isn’t always the best choice. For example,\n",
    "if you have a small dataset, or if your data is imbalanced (i.e., one class has many more samples than another),\n",
    "then simple random splitting may result in training or testing sets that don’t accurately represent \n",
    "the overall distribution of data1. In such cases, other methods like stratified sampling might be more appropriate1.\n",
    "Also, when dealing with time series data (like stock prices or weather data),\n",
    "it’s often more appropriate to split the data based on a *chronological* order rather than randomly1.\n",
    "This helps in evaluating the model’s performance on future unseen dat\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5056970117130049\n",
      "0.028567611540622706\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_train ,y_train))\n",
    "print(model.score(x_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree =2) # حددنا عايزينه يكون كام احتمال ويرفع لانهي درجه \n",
    "x_train_poly= poly.fit_transform(x_train) \n",
    "'''\n",
    "The fit_transform method does two things.\n",
    "First, it calculates the parameters needed for the transformation from the training data \n",
    "(in this case, it identifies the number of features in x_train). \n",
    "Then, it applies the transformation to the training data. The transformation involves creating all polynomial \n",
    "combinations of the features up to the second degree.\n",
    " The transformed data is stored in x_train_poly.\n",
    "\n",
    " \"اللهم اجعل كل حرف كتبته وفهمته شفيعا لي يوم القيامة وذكرني بيه عند الحاجة اليه يارب عشان بنسي جامد :\"\n",
    "\n",
    "'''\n",
    "\n",
    "x_test_poly=poly.transform(x_test)  \n",
    "#The transform method applies the previously calculated parameters to the test data. \n",
    "#It’s important to note that we’re not fitting the PolynomialFeatures instance to the test data.\n",
    "#We’re only transforming the test data based on the parameters learned from the training data. \n",
    "#This is to prevent data leakage from the test set into the training set.\n",
    "# The transformed test data is stored in x_test_poly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It’s important to note that we’re not fitting the PolynomialFeatures instance to the test data.\n",
    "# We’re only transforming the test data based on the parameters learned from the training data. \n",
    "# This is to prevent data leakage from the test set into the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preprocessing before splitting***: If you preprocess your data (like normalization, augmentation, dimensionality reduction, or feature selection) before splitting it into training and test sets, you’re leaking information from the test set into the training set2. ***This is because the preprocessing steps are supposed to be learned from the training data alone.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "***difference between applying PolynomialFeatures on a dataset before and after splitting:***\n",
    "\n",
    "**[Applying PolynomialFeatures Before Splitting:]** \n",
    "When you apply PolynomialFeatures to your entire dataset before splitting into training and test sets, **you’re transforming all of your data at once**. This means that the polynomial features are created based on the entire dataset. However, ***this approach can lead to data leakage***. Data leakage occurs when information from the test set leaks into the training set. In this case, ***because the polynomial features are created based on the entire dataset, they may contain information about the test set that could end up in the training set***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*** For example, if you normalize the entire dataset before splitting, the mean and standard deviation used for normalization will be calculated using both the training and test data. This means that the normalization process has information about the test data, which can leak into the training data***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**[Applying PolynomialFeatures After Splitting]**: \n",
    "When you apply PolynomialFeatures after splitting your data into training and test sets, you’re ensuring that the polynomial features are **created independently for each set**. This means that the polynomial features in the training set are based only on the information in the training set, and the polynomial features in the test set are based only on the information in the test set(تمام). This approach helps to prevent data leakage, as the training set does not have any information about the test set\n",
    "\n",
    "# مراجعة سريعة علي polynomial :)\n",
    "\n",
    "*** It generates a new matrix of features by creating all polynomial combinations of the original features up to a specified degree ex 2.\n",
    "\n",
    "For example, if you have two features a and b, and you apply PolynomialFeatures with degree 2, it will generate the following features: 1, a, b, a^2, ab, b^2. This is because it includes all polynomial combinations of a and b up to the second degree2.\n",
    "\n",
    "**The degree of the polynomial is used to control the number of features added()**. A degree of d will add d new variables for each input variable(تمام ). However, it’s unusual to use d greater than 3 or 4 because for large values of d, the polynomial curve can become overly flexible(يشطح) and can take on some very strange shape (lol)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***To avoid data leakage, it’s recommended to split the data first and then apply preprocessing steps separately to the training and test sets. For example, you would calculate the mean and standard deviation on the training data alone, and use these values to normalize both the training and test data***\n",
    "\n",
    "# normalize the data:\n",
    "\n",
    "When we say “normalize the data”, we’re usually talking about scaling the data so that it has a mean of 0 and a standard deviation of 1. This is also known as **standardization**.\n",
    "\n",
    "The process involves two steps:\n",
    "\n",
    "1-Calculate the mean and standard deviation: **These calculations should be done using the training data only**. This is because the training data is what we have available at the time of model training. *We don’t want to include information from the test data at this stage,* as that would be a form of data leakage.\n",
    "\n",
    "\n",
    "2_Scale the data: Subtract the mean and divide by the standard deviation for each data point in the training data. This scales the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "Now, when it comes time to apply the model to the test data, **we want to scale the test data in the same way that we scaled the training data**. ***However, we don’t want to calculate new mean or standard deviation values from the test data. Instead, we use the mean and standard deviation values that we calculated from the training data. This ensures that the scaling is consistent between the training data and the test data.***\n",
    "\n",
    "So, when we say “use these values to normalize both the training and test data”,*** we mean that the same mean and standard deviation values (calculated from the training data) should be used to scale both the training data and the test data. This helps to ensure that the model treats the training and test data consistently, and it helps to prevent data leakage.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
